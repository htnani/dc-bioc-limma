---
title: "Understanding linear models"
author: "John Blischak"
date: "2018-08-20"
output: workflowr::wflow_html
---

## Introduction

The goal is to provide some visualizations to help you understand linear models.

## Setup

```{r packages, message=FALSE}
library("broom")
library("cowplot")
library("dplyr")
library("ggplot2")
theme_set(theme_classic(base_size = 16))
library("knitr")
opts_chunk$set(fig.width = 10, fig.height = 5, message = FALSE)
```

## Visualize differential expression

Visualize boxplots of a gene that is clearly differentially expressed (a) and
one that is unclear due to increased variance (b).

```{r boxplots}
df_vis <- data.frame(status = rep(c("con", "treat"), each = 50)) %>%
  mutate(gene_de = c(rpois(n() / 2, lambda = 12), rpois(n() / 2, lambda = 30)),
         gene_var =c(rpois(n() / 2, lambda = 15) + rnorm(n() / 2, sd = 10),
                     rpois(n() / 2, lambda = 25) + rnorm(n() / 2, sd = 10)))
box_de <- ggplot(df_vis, aes(x = status, y = gene_de)) +
  geom_boxplot() +
  theme_classic(base_size = 16) +
  ylim(0, 40) +
  labs(x = "Treatment status", y = "Gene expression level",
       title = "Differential expression")
box_var <- ggplot(df_vis, aes(x = status, y = gene_var)) +
  geom_boxplot() +
  theme_classic(base_size = 16) +
  ylim(0, 40) +
  labs(x = "Treatment status", y = "Gene expression level",
       title = "High variance")
plot_grid(box_de, box_var, labels = letters[1:2])
```

## Create linear model

As you just visualized, differential expression describes the
situation in which a gene has a different mean expression level between
conditions. While some gene expression patterns are easily diagnosed as
differential expression or not from a quick visualization, you also saw some
examples that were more ambiguous. Furthermore, you need a method that is more
robust than a quick visual inspection and also scales to thousands of genes. For
this you will use the tools of statistical inference to determine if the
difference in mean expression level is larger than that expected by chance.
Specifically, you will use linear models to perform the hypothesis tests. Linear
models are an ideal choice for genomics experiments because their flexibility
and robustness to assumptions allow you to conveniently model data from various
study designs and data sources.

You should have already been introduced to linear models, for example in a
DataCamp course such as [Correlation and Regression][dc-regression], or in an
introductory statistics course. Here I'll review the terminology we will use in
the remainder of the course, how to specify a linear model in R, and the
intuition behind linear models.

[dc-regression]: https://www.datacamp.com/courses/correlation-and-regression

$$ Y =  \beta_0 + \beta_1 X_1 + \epsilon $$

In this equation of a linear model, Y is the response variable. It must be a
continuous variable. In the context of differential expression, it is a relative
measure of either RNA or protein expression level for one gene. $X_1$ is an
explanatory variable, which can be continuous or discrete, for example, control
group versus treatment, or mutant versus wild type. $\beta_1$ quantifies the
effect of the explanatory variable on the response variable. Furthermore, we can
add additional explanatory variables to the equation for more complicated
experimental designs. Lastly, \epsilon models the random noise in the
measurements.

In R, you specify a linear model with the function `lm`. This uses R's formula
syntax. On the left is the object that contains the response variable, and to
the right of the tilde are the objects that contain the explanatory variables.

```{r eval=FALSE}
lm(y ~ x1)
```

A second explanatory variable can be added with a plus sign.

$$ Y =  \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon $$

```{r eval=FALSE}
lm(y ~ x1 + x2)
```

## Building some intuition via simulation

The simulation below demonstrates how the statistical significance of the
computed F-statistic of a linear model is equally affected by the noise (the
residual sum of squares) and the signal (the explained sum of squares).

```{r inuition}
# Simulate a linear regression.
#
# n = sample size
# effect = slope
# error = standard deviation of distribution of residuals
# seed = seed for random number generator
#
# Returns a data.frame with the following columns:
#
# x          Explanatory variable
# y          Response variable
# y_bar      Mean of response variable
# intercept  Intercept of least squares regression line
# slope      Slope of least squares regression line
# y_hat      Fitted values
# fstat      F-statistic
# ss_exp     Explained sum of squares
# ss_res     Residual sum of squares (noise)
sim_lm <- function(n, effect, error, seed = 1) {
  stopifnot(is.numeric(n), n > 0, length(n) == 1)
  stopifnot(is.numeric(effect), length(effect) == 1)
  stopifnot(is.numeric(error), error > 0, length(error) == 1)
  stopifnot(is.numeric(seed), length(seed) == 1)
  
  set.seed(seed)
  x = runif(n, min = -25, max = 25)
  y = x * effect + rnorm(n, sd = error)
  y_bar = mean(y)
  mod <- lm(y ~ x)
  coefs <- coef(mod)
  intercept <- coefs[1]
  slope <- coefs[2]
  y_hat = fitted(mod)
  anova_tidy <- tidy(anova(mod))
  fstat <- anova_tidy$statistic[1]
  ss <- anova_tidy$sumsq
  ss_exp <- ss[1]
  ss_res <- ss[2]
  stopifnot(ss_exp - sum((y_hat - y_bar)^2) < 0.01)
  stopifnot(ss_res - sum(residuals(mod)^2) < 0.01)
  return(data.frame(x, y, y_bar, intercept, slope, y_hat, fstat, ss_exp, ss_res,
                    row.names = 1:n))
}

# Visualize the residual sum of squares
plot_ss_res <- function(d) {
  ggplot(d, aes(x = x, y = y)) +
    geom_point() +
    geom_abline(aes(intercept = intercept, slope = slope)) +
    geom_linerange(aes(ymin = y, ymax = y_hat), color = "red",
                   linetype = "dashed") +
    geom_text(aes(x = quantile(x, 0.25), y = quantile(y, 0.75),
                  label = round(ss_res, 2)), color = "red") +
    labs(title = "Residual sum of squares (noise)") +
    ylim(-60, 60)
}

# Visualize the explained sum of squares
plot_ss_exp <- function(d) {
  ggplot(d, aes(x = x, y = y)) +
    geom_abline(aes(intercept = intercept, slope = slope)) +
    geom_hline(aes(yintercept = y_bar)) +
    geom_linerange(aes(ymin = y_hat, ymax = y_bar), color = "blue",
                   linetype = "dashed") +
    geom_text(aes(x = quantile(x, 0.25), y = quantile(y, 0.75),
                  label = round(ss_exp, 2)), color = "blue") +
    labs(title = "Explained sum of squares") +
    ylim(-60, 60)
}
```

## Simulation: Baseline

```{r baseline}
# baseline
baseline <- sim_lm(n = 10, effect = 2, error = 5)
baseline_ss_res <- plot_ss_res(baseline)
baseline_ss_exp <- plot_ss_exp(baseline)
plot_grid(baseline_ss_res, baseline_ss_exp)
baseline$fstat[1]
```

The baseline simulation has an F-statistic of `r baseline$fstat[1]`.

## Simulation: Increased error

```{r increased-error}
# Increased error
more_error <- sim_lm(n = 10, effect = 2, error = 10)
more_error_ss_res <- plot_ss_res(more_error)
more_error_ss_exp <- plot_ss_exp(more_error)
plot_grid(more_error_ss_res, more_error_ss_exp)
more_error$fstat[1]
```

Doubling the error decreases the test statistic by a factor of 4.

## Simulation: Decreased signal

```{r decreased-signal}
# Decreased signal
less_signal <- sim_lm(n = 10, effect = 1, error = 5)
less_signal_ss_res <- plot_ss_res(less_signal)
less_signal_ss_exp <- plot_ss_exp(less_signal)
plot_grid(less_signal_ss_res, less_signal_ss_exp)
less_signal$fstat[1]
```

Similarly, halving the signal decreases the test statistic by a factor of 4.
